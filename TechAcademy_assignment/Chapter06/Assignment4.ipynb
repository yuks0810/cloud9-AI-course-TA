{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b0d37628-d11d-43ca-bd70-44567aebbf41"
    }
   },
   "source": [
    "<H2>課題4: 評判分析</H2>\n",
    "\n",
    "<p>本課題ではAmazonに投稿された映画のレビュー(英語)を分析し、レビューがPositiveかNegativeかの判別を行います。</p>\n",
    "<p>Amazon_reviewファイルにはTraining dataとTest dataが入っています。</p>\n",
    "<p>Training dataを用いて機械学習を行い、その結果を元に6つのTest dataがpositiveかNegativeかを判別してください。</p>\n",
    "<p>6章で学んだ内容を踏まえ、各セルに'#コメント'の内容を実行するコードを記入してください。</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b14e9b2d-e81d-4c46-aaa5-a21ead865efb"
    }
   },
   "source": [
    "<H2>1. 必要なモジュールの読み込み</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import numpy as np_amazon\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn import svm, naive_bayes\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>2. ディレクトリの確認</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__MACOSX', 'Amazon_review', 'Assignment4.ipynb', '.ipynb_checkpoints', 'Amazon_review-20180604.zip', 'Untitled.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# カレントディレクトリの確認\n",
    "import os\n",
    "print(os.listdir(os.path.normpath(\"./\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Training_data', '_DS_Store', 'Test_data']\n"
     ]
    }
   ],
   "source": [
    "# データディレクトリの確認\n",
    "print(os.listdir(\"./Amazon_review/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>3. Dataの読み込み </H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォルダ中のテキストへのパスを取得した上で各ファイルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your defauld locale is None\n",
      "Your locale is set as ja_JP.UTF-8\n"
     ]
    }
   ],
   "source": [
    "# default locale を ja_JP.UTF-8に設定する\n",
    "def set_locale():\n",
    "    default = os.environ.get('LC_ALL')\n",
    "    print(\"Your defauld locale is\", default)\n",
    "    if default is None:\n",
    "        os.environ.setdefault('LC_ALL', 'ja_JP.UTF-8')\n",
    "        print(\"Your locale is set as ja_JP.UTF-8\")\n",
    "        \n",
    "set_locale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globでディレクトリを検索し、フォルダ中のテキストのパスを取得 (Training set)\n",
    "\n",
    "import glob\n",
    "\n",
    "DATA_PATH = \"./Amazon_review/Training_data/\"\n",
    "\n",
    "neg_files = glob.glob(\"{0}neg/*\".format(DATA_PATH))\n",
    "pos_files = glob.glob(\"{0}pos/*\".format(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Amazon_review/Training_data/neg/cv689_tok-12427.txt', './Amazon_review/Training_data/neg/cv142_tok-10859.txt']\n",
      "['./Amazon_review/Training_data/pos/cv620_tok-13475.txt', './Amazon_review/Training_data/pos/cv413_tok-23923.txt']\n"
     ]
    }
   ],
   "source": [
    "# 相対パスで各テキストデータへのパスが格納されていることを確認\n",
    "print(neg_files[0:2])\n",
    "print(pos_files[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globでディレクトリを検索し、フォルダ中のテキストのパスを取得 (Test set)\n",
    "import glob\n",
    "\n",
    "DATA_PATH = \"./Amazon_review/Test_data/\"\n",
    "\n",
    "neg_files_test = glob.glob(\"{0}neg/*\".format(DATA_PATH))\n",
    "pos_files_test = glob.glob(\"{0}pos/*\".format(DATA_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Amazon_review/Training_data/neg/cv689_tok-12427.txt', './Amazon_review/Training_data/neg/cv142_tok-10859.txt']\n",
      "['./Amazon_review/Training_data/pos/cv620_tok-13475.txt', './Amazon_review/Training_data/pos/cv413_tok-23923.txt']\n",
      "['./Amazon_review/Test_data/neg/amazon_review_10002.txt', './Amazon_review/Test_data/neg/amazon_review_10000.txt']\n",
      "['./Amazon_review/Test_data/pos/amaozn_review_20000.txt', './Amazon_review/Test_data/pos/amaozn_review_20001.txt']\n"
     ]
    }
   ],
   "source": [
    "# 相対パスで各テキストデータへのパスが格納されていることを確認\n",
    "print(neg_files[0:2])\n",
    "print(pos_files[0:2])\n",
    "\n",
    "print(neg_files_test[0:2])\n",
    "print(pos_files_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  各ファイルを読み込む関数を定義\n",
    "import sys\n",
    "\n",
    "def text_reader(file_path):\n",
    "    python_version = sys.version_info.major\n",
    "    \n",
    "    if python_version >= 3:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                  for line in f:\n",
    "                      print(line)\n",
    "                  \n",
    "    else:\n",
    "        with open(file_path, 'r') as f:\n",
    "                  for line in f:\n",
    "                      print(line)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’ve never been a Marvel Cinematic Universe fan; back in May 2012, the midnight premiere of The Avengers before I became a huge cinema buff, the only addition of the series I saw was Iron Man, which I felt no desire to watch again. I only bought a ticket to The Avengers because some friends invited me. Afterward, virtually the whole world went nuts and called it the greatest motion picture ever, I however just saw horrific, nauseous action void of artistic purpose. While softer attitudes towards Marvel eventually came my way, people praising ridiculous junk food over quality art still sickens me.\n",
      "\n",
      "\n",
      "\n",
      "Even today, Marvel representing the early bane of my taste in movies still affects my outlook upon Avengers: Infinity War. Like Rocket Raccoon’s mockery upon anybody different, Marvel’s corporate heads blare their red logo in its abused emotional manipulation, whilst their followers turn a deep azure hue in moral discouragement.\n",
      "\n",
      "\n",
      "\n",
      "This incorporation of many characters turns toilsome on a full bladder, since long stretches between subplots average between 20-40 minutes until returned to. The editors couldn’t keep them consistently active, but sure, they got enough time for infinite out-of-place jokes! Thor’s first scene with the Guardians alone contains nonstop laughs increasing the total runtime threefold. Gags continue throughout serious beats, particularly one where Drax snacking on chips interrupts a sad romantic exchange.\n",
      "\n",
      "\n",
      "\n",
      "There are plenty… PLENTY of other flaws that ruin other evocative moments; whenever two combatants start punching and kicking, feeble battle choreographed by eye-sore camera movements looks set on anything besides vengeance. Rage seldom reaches full capacity, since no actors stayed on the same page: One aims to be optimistic (Chris Pratt), another aims to be deadpan (Robert Downey Jr.), and another cannot decide (Elizabeth Olsen), all cold in believability. Chadwick Boseman should take most of the blame as he continues his noncombative blank stares, alongside Chris Hemsworth as his obligatory smile disrupts any flow.\n",
      "\n",
      "\n",
      "\n",
      "Here’s a clear perspective against the overhype of such a gimmicky unforeseen divergence from Marvel. Last Friday, Avengers: Infinity War skyrocketed up to #10 on IMDb’s Top 250, bumping up to #9 on Sunday (though it’s back at #10 now), surpassing genuinely good films: Saving Private Ryan, the Star Wars original trilogy, The Green Mile, Amadeus, and even Citizen Kane!\n",
      "\n",
      "\n",
      "\n",
      "These classics knew how to present new ideas in clever ways, something the Russo Brothers failed at. Nothing they did here is as clever as you would want to think… the climax alone rips off The Lord of the Rings, an enduring trilogy about healthy retaliation, in its grand epic scale. Now, I understand Marvel does honor Stan Lee’s created universe enough to allow greater depth than previous superhero establishments, except those corporate heads beneath Disney’s control still insult the art of filmmaking by taking advantage of consumers for the sake of bank.\n",
      "\n",
      "\n",
      "\n",
      "Instead of futile cash-bait, imagine if we celebrated genuinely intelligent narrative commentaries on true problems? Or better yet, stopped hating on DC fanboys because of foolish loyalty to nonexistent people with abilities we could never hope to obtain? Then I can guarantee you that the world would become a much kinder place without the need for a dictatorial jerk in a dumb America costume telling us what’s important in life.\n"
     ]
    }
   ],
   "source": [
    "# 読み込んだファイルの中身を表示（Test data, Negative, 1ファイル目）\n",
    "\n",
    "text_reader(neg_files_test[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> 4.特徴ベクトルの作成</H2>\n",
    "<p>特徴ベクトルの作成に必要な関数を定義した後に特徴ベクトルを作成します。</p>\n",
    "<p>特徴ベクトルはTraining setとTest setで同じ要素を持つ必要がありますので、両方のsetを合わせて作成し、後でTraining setとTest setに分けます。</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各単語の出現回数をカウントする関数を定義\n",
    "def word_counter(string):\n",
    "    words = string.strip().split()\n",
    "    count_dict = collections.Counter(words)\n",
    "    return dict(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigramを作成する関数を定義\n",
    "def get_unigram(file_path):\n",
    "    result = []\n",
    "    python_version = sys.version_info.major\n",
    "    \n",
    "    if python_version >= 3:\n",
    "        for file in file_path:\n",
    "            with open(file, 'r', encoding='utf=8') as f:\n",
    "                for line in f:\n",
    "                    count_dict = word_counter(line)\n",
    "                    result.append(count_dict)\n",
    "                    \n",
    "    else:\n",
    "        for file in file_path:\n",
    "            with open(file, 'r') as f:\n",
    "                for line in f:\n",
    "                    count_dict = word_counter(line)\n",
    "                    result.append(count_dict)\n",
    "                    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "# Unigramをリストアップ\n",
    "# Training用Positive、Negativeデータ、Test用Positiv、Negativeデータから、Unigramを取得し、ひとつの変数へ格納する\n",
    "\n",
    "\n",
    "%time\n",
    "\n",
    "DATA_NUM = 700\n",
    "\n",
    "unigrams_data = get_unigram(neg_files[:DATA_NUM]) + get_unigram(pos_files[:DATA_NUM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'united': 1, 'states': 1, ',': 53, '1998': 1, 'u': 2, '.': 36, 's': 4, 'release': 1, 'date': 2, ':': 13, '5/8/98': 1, '(': 7, 'wide': 1, ')': 7, 'running': 1, 'length': 1, '1': 3, '20': 1, 'mpaa': 1, 'classification': 1, 'r': 1, 'profanity': 1, 'mature': 1, 'themes': 1, 'sexual': 1, 'situations': 2, 'theatrical': 1, 'aspect': 1, 'ratio': 1, '85': 1, 'cast': 1, 'jada': 2, 'pinkett': 6, 'smith': 4, 'tommy': 2, 'davidson': 2, 'dave': 1, 'chappelle': 1, 'paula': 1, 'jai': 1, 'parker': 1, 'll': 1, 'cool': 1, 'j': 1, 'darrel': 1, 'heath': 1, 'michael': 2, 'ralph': 1, 'duane': 1, 'martin': 1, 'director': 1, 'daisy': 2, 'v': 2, 'mayer': 2, 'producers': 1, 'beth': 1, 'hubbard': 2, 'screenplay': 1, 'david': 2, 'c': 2, 'johnson': 1, 'cinematography': 1, 'jean': 1, 'lepine': 1, 'music': 1, 'michel': 1, 'colombier': 1, 'distributor': 1, 'new': 1, 'line': 1, 'cinema': 1, 'working': 1, 'in': 10, 'the': 33, 'motion': 2, 'picture': 1, 'industry': 1, 'must': 1, 'be': 4, 'a': 28, 'constant': 2, 'source': 1, 'of': 16, 'frustration': 1, 'for': 8, 'front-line': 1, 'african': 1, 'american': 1, 'actress': 1, 'like': 1, 'despite': 2, 'being': 1, 'one': 3, 'freshest': 1, 'talents': 1, 'available': 2, 'has': 2, 'often': 1, 'been': 5, 'relegated': 1, 'to': 16, 'playing': 1, 'thankless': 1, 'supporting': 2, 'parts': 1, 'low': 2, 'down': 1, 'dirty': 1, 'shame': 1, 'nutty': 1, 'professor': 1, 'problem': 1, 'is': 11, 'course': 1, 'that': 10, 'there': 2, \"aren't\": 2, 'many': 1, 'good': 2, 'roles': 1, 'black': 2, 'women': 1, 'take': 1, 'away': 1, 'likes': 1, 'waiting': 1, 'exhale': 1, 'set': 1, 'it': 5, 'off': 2, 'soul': 1, 'food': 1, 'and': 11, \"eve's\": 1, 'bayou': 1, 'all': 4, \"that's\": 2, 'left': 1, 'chance': 1, \"someone's\": 1, 'girlfriend': 1, 'local': 1, 'whore': 1, 'or': 2, 'murder': 1, 'victim': 1, 'as': 2, 'result': 1, \"smith's\": 2, 'first': 2, 'opportunity': 2, 'atop': 1, 'marquee': 1, 'she': 2, 'stuck': 1, 'stupid': 2, 'formulaic': 1, 'romantic': 2, 'comedy': 1, 'with': 6, 'unpromising': 1, 'title': 1, 'woo': 9, 'actually': 1, 'latest': 2, 'directorial': 1, 'effort': 2, 'from': 3, 'party': 1, 'girl': 1, 'could': 1, 'have': 3, 'worse': 2, 'than': 1, 'film': 1, 'offers': 1, 'few': 1, 'funny': 1, 'albeit': 1, 'juvenile': 1, 'moments': 1, 'on-screen': 1, 'relationship': 2, 'between': 1, \"davidson's\": 1, 'tim': 4, 'appealing': 1, 'on': 2, 'those': 2, 'rare': 1, 'occasions': 1, 'when': 4, 'two': 1, 'forced': 1, 'play': 1, 'dumb': 1, 'sake': 1, 'an': 4, 'endless': 1, 'barrage': 1, 'cheap': 1, 'gags': 1, 'unfortunately': 1, 'huge': 2, 'portions': 1, 'movie': 1, 'are': 6, 'insulting': 1, 'intelligence': 1, 'anyone': 1, 'triple-digit': 1, 'i': 3, 'q': 1, 'painfully': 1, 'contrived': 1, 'main': 1, 'characters': 2, 'lifted': 1, 'right': 2, 'out': 2, 'sit-coms': 1, 'players': 1, 'so': 3, 'incredibly': 1, 'wondered': 1, 'whether': 1, 'they': 2, 'participated': 1, 'some': 1, 'kind': 1, '\"': 8, 'free': 1, 'lobotomy': 1, 'lab': 1, 'experiment': 1, 'spontaneous': 1, 'energetic': 1, 'young': 1, 'woman': 1, 'who': 3, 'looking': 1, 'love': 3, 'wrong': 1, 'places': 1, 'her': 2, 'testosterone-overdosed': 1, 'drug-dealer': 1, 'came': 1, 'abrupt': 1, 'end': 2, 'refused': 1, 'wear': 1, 'beeper': 1, 'now': 1, 'transvestite/medium': 1, 'friend': 1, 'celestrial': 1, 'girlina': 1, 'predicts': 1, \"she's\": 1, 'about': 2, 'meet': 1, 'mr': 1, \"he'll\": 1, 'virgo': 2, 'later': 1, 'day': 1, 'given': 1, 'go': 1, 'blind': 1, 'shy': 1, 'man': 1, 'penchant': 1, 'neatness': 1, 'order': 1, 'just': 1, 'happens': 1, \"it's\": 2, 'not': 1, 'at': 2, 'sight': 1, 'however': 1, 'things': 1, 'get': 3, 'rocky': 1, 'start': 1, 'makes': 1, 'awkward': 1, 'pass': 1, 'even': 1, 'pair': 1, 'arrives': 1, 'swanky': 1, 'restaurant': 1, 'enjoy': 2, 'quiet': 1, 'dinner': 1, 'one-hundred': 1, 'year': 1, 'history': 1, 'pictures': 1, 'ton': 1, 'bad': 1, 'white': 1, 'movies': 1, 'mismatched': 1, 'lovers': 1, 'suppose': 1, 'only': 1, 'fair': 1, 'we': 2, 'same': 1, 'caliber': 1, 'comedies': 1, 'know': 1, 'beginning': 1, 'going': 1, 'up': 2, 'together': 1, 'question': 1, 'their': 2, 'courtship': 1, 'rituals': 1, 'entertaining': 1, '?': 1, 'entry': 1, 'genre': 1, 'audience': 1, 'becomes': 1, 'caught': 1, 'story': 1, 'no': 1, 'matter': 1, 'how': 2, 'familiar': 1, 'reliance': 1, 'upon': 1, 'unfunny': 1, 'moronic': 1, 'humor': 1, 'sinks': 1, 'project': 1, 'spotting': 1, 'continuity': 1, 'gaffes': 1, \"there's\": 2, 'involving': 1, 'corvette': 1, 'broken': 1, 'window': 1, 'miraculously': 1, 'repairs': 1, 'itself': 1, 'added': 1, 'mystery': 1, 'car': 1, 'looses': 1, 'its': 1, 'top': 1, 'sadly': 1, \"film's\": 1, 'most': 1, 'enjoyable': 1, 'sequences': 1, 'entertainment': 1, 'value': 1, 'entirely': 1, 'unintentional': 1, 'exuding': 1, 'charm': 1, 'every': 1, 'pore': 1, 'little': 1, 'can': 1, 'do': 1, 'save': 1, \"johnson's\": 1, 'script': 1, 'since': 1, 'misses': 1, 'mark': 1, 'by': 1, 'much': 1, 'better': 1, 'name': 1, 'might': 1, 'whoops': 1}\n",
      "data size: 0.011264 [MB]\n"
     ]
    }
   ],
   "source": [
    "# Unigramの内容を確認 (Training set)\n",
    "print(unigrams_data[0])\n",
    "print('data size:', sys.getsizeof(unigrams_data) / 1000000, \"[MB]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "# DictVectorizerを用いてUnigramを行列の形に変形　（各行が1つのレビュー、各列が単語、要素がその単語の出現数）\n",
    "\n",
    "%time\n",
    "vec=DictVectorizer()\n",
    "feature_vectors_csr = vec.fit_transform(unigrams_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1400x44219 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 496525 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 作成したデータを確認\n",
    "feature_vectors_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.ラベルデータの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回扱うデータセットは全てに negative, positive というラベルが振られています。<br>\n",
    "ここではそのラベルを negative → 0, positive → 1 とすることで二値判別問題のセットアップを構築します。<br>\n",
    "ラベルも特徴ベクトルと同様にTraining setとTest setを合わせて作成し、後でTraining setとTest　setに分けます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベルデータの作成\n",
    "labels = np.r_[np.tile(0, DATA_NUM), np.tile(1, DATA_NUM)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 1\n"
     ]
    }
   ],
   "source": [
    "#正しい位置で0と1の振替がなされているか確認 (Training setの0と1、Test setの0と1)\n",
    "print(labels[0], labels[DATA_NUM-1], labels[DATA_NUM], labels[2*DATA_NUM-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> 6. 学習用データと学習評価用データの作成</H2>\n",
    "<p>今回はTree fold cross validationを行いますので、まずTraining setを3分割します。</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回はThree fold cross validation でモデルを評価\n",
    "np.random.seed(7789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seedを固定し、乱数を1400個作成\n",
    "shuffle_order = np.random.choice(2*DATA_NUM, 2*DATA_NUM, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 1400\n",
      "first 10 elements: [1235 1232  910  162  343 1160  221  545 1112 1322]\n"
     ]
    }
   ],
   "source": [
    "#生成した乱数の中身を確認\n",
    "print(\"length:\", len(shuffle_order))\n",
    "print(\"first 10 elements:\", shuffle_order[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one third of the length: 466\n",
      "#of '1' in 1st set: 227\n",
      "#of '1' in 2st set: 233\n",
      "#of '1' in 3st set: 240\n"
     ]
    }
   ],
   "source": [
    "#作成した乱数を元にTraining setを3分割する\n",
    "one_third_size = int(2*DATA_NUM/3.)\n",
    "print(\"one third of the length:\", one_third_size)\n",
    "\n",
    "print(\"#of '1' in 1st set:\", np.sum(labels[ shuffle_order[:one_third_size]]))\n",
    "print(\"#of '1' in 2st set:\", np.sum(labels[ shuffle_order[one_third_size:2*one_third_size]]))\n",
    "print(\"#of '1' in 3st set:\", np.sum(labels[ shuffle_order[2*one_third_size:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 学習に必要な関数の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 与えられたリストをN分割する関数<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与えられたリストをN分割する関数を定義\n",
    "def N_splitter(seq, N):\n",
    "    avg = len(seq) / float(N)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "    \n",
    "    while last < len(seq):\n",
    "        out.append( seq[int(last):int(last + avg)])\n",
    "        \n",
    "        return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 動作を確認\n",
    "N_splitter(range(14), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>2. train_model : 説明変数とラベルと手法を与えることでモデルを学習する</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model \n",
    "def train_model(features, labels, method='SVM', parameters=None):\n",
    "    \n",
    "    if method == 'SVM':\n",
    "        model = svm.SVC()\n",
    "    elif method == 'NB':\n",
    "        model = naive_bayes.GaussianNB()\n",
    "    elif method == 'RF':\n",
    "        model = RandomFOrestClassifier()\n",
    "    else:\n",
    "        print(\"Set method as SVM (for Support vector machine), NB(for Naive Bayes) or RF\")\n",
    "    if parameters:\n",
    "        model.set_params(**parameters)\n",
    "    model.fit(features, labels)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>3. predict : モデルと説明変数を与えることでラベルを予測する</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "def predict(model, features):\n",
    "    predictions = model.predict(features)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>4. evaluate_model : 予測したラベルと実際の答えの合致数を調べる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_model \n",
    "def evaluate_model(predictions, labels):\n",
    "    data_num = len(labels)\n",
    "    correct_num=np.sum(predictins == labels)\n",
    "    return data_num, correct_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> 5. cross_validate : cross_validationを実行する </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate\n",
    "def cross_validate(n_folds, feature_vectors, labels, shuffle_order, method='SVM', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 学習の実施、精度の検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingを実施\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を表示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. パラメータのチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パラメータをチューニング（どのような形でも構いません）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チューニング後のパラメータで学習\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を表示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test dataを用いてモデルを評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上記で作成したモデルを用いてTest dataの評価を実施\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
